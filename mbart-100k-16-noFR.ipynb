{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-02T10:32:09.577754Z",
     "iopub.status.busy": "2024-10-02T10:32:09.576937Z",
     "iopub.status.idle": "2024-10-02T10:32:22.459276Z",
     "shell.execute_reply": "2024-10-02T10:32:22.458085Z",
     "shell.execute_reply.started": "2024-10-02T10:32:09.577707Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets sacrebleu sentencepiece --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T10:32:22.461551Z",
     "iopub.status.busy": "2024-10-02T10:32:22.461228Z",
     "iopub.status.idle": "2024-10-02T10:32:22.466852Z",
     "shell.execute_reply": "2024-10-02T10:32:22.465768Z",
     "shell.execute_reply.started": "2024-10-02T10:32:22.461519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T10:32:22.468260Z",
     "iopub.status.busy": "2024-10-02T10:32:22.467941Z",
     "iopub.status.idle": "2024-10-02T10:32:42.731807Z",
     "shell.execute_reply": "2024-10-02T10:32:42.731048Z",
     "shell.execute_reply.started": "2024-10-02T10:32:22.468230Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T10:32:42.733421Z",
     "iopub.status.busy": "2024-10-02T10:32:42.732826Z",
     "iopub.status.idle": "2024-10-02T10:32:42.768522Z",
     "shell.execute_reply": "2024-10-02T10:32:42.767441Z",
     "shell.execute_reply.started": "2024-10-02T10:32:42.733379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T10:32:42.771623Z",
     "iopub.status.busy": "2024-10-02T10:32:42.771309Z",
     "iopub.status.idle": "2024-10-02T10:32:51.059786Z",
     "shell.execute_reply": "2024-10-02T10:32:51.058854Z",
     "shell.execute_reply.started": "2024-10-02T10:32:42.771592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b17445cf7a64be6a4752e6c431cf9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/135M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61615dfdea2484c99240b6f6043fc50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/12.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d56a0ff42f40ac8f4c5d6cf3f14718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsinlu-repo/validation/0000.parquet:   0%|          | 0.00/242k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f66e454245d4f959d812f7111228934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1621665 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be7781f10894909bf601aff1d85efc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/48359 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c82359df1b45caaef6eade1f503ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2137 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"persiannlp/parsinlu_translation_en_fa\", cache_dir=\"./cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T10:32:51.061290Z",
     "iopub.status.busy": "2024-10-02T10:32:51.060970Z",
     "iopub.status.idle": "2024-10-02T10:32:51.761192Z",
     "shell.execute_reply": "2024-10-02T10:32:51.760214Z",
     "shell.execute_reply.started": "2024-10-02T10:32:51.061258Z"
    }
   },
   "outputs": [],
   "source": [
    "max_samples = 100000\n",
    "\n",
    "train_dataset = dataset['train'].shuffle(seed=42).select(range(max_samples))\n",
    "# validation_dataset = dataset['validation'].shuffle(seed=42).select(range(int(max_samples * 0.1)))\n",
    "validation_dataset = dataset['validation'].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T10:32:51.762787Z",
     "iopub.status.busy": "2024-10-02T10:32:51.762479Z",
     "iopub.status.idle": "2024-10-02T10:32:51.769480Z",
     "shell.execute_reply": "2024-10-02T10:32:51.768489Z",
     "shell.execute_reply.started": "2024-10-02T10:32:51.762754Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [f\"translate English to Persian: {source}\" for source in examples['source']]\n",
    "\n",
    "    targets = [target[0] if isinstance(target, list) else target for target in examples['targets']]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=64, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=64, truncation=True, padding=\"max_length\").input_ids\n",
    "\n",
    "\n",
    "    labels = [[(label if label != tokenizer.pad_token_id else -100) for label in label_set] for label_set in labels]\n",
    "\n",
    "\n",
    "    model_inputs[\"labels\"] = labels\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T10:32:51.770853Z",
     "iopub.status.busy": "2024-10-02T10:32:51.770572Z",
     "iopub.status.idle": "2024-10-02T10:33:07.855959Z",
     "shell.execute_reply": "2024-10-02T10:33:07.855013Z",
     "shell.execute_reply.started": "2024-10-02T10:32:51.770824Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caef605b13d34f2396ae69db8050d112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38f9b4ec14e413392b224f2b089f150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94477e6d54264e6f83c2560737a01c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d67bd55bd5476e99d0fa4be74bddc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70bba4288d34e47a17e71271cdf8814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da11433036a848a88bf292c326720ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MBartForConditionalGeneration(\n",
       "  (model): MBartModel(\n",
       "    (shared): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "    (encoder): MBartEncoder(\n",
       "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartEncoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): MBartDecoder(\n",
       "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartDecoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250054, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'facebook/mbart-large-50-many-to-many-mmt'\n",
    "\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name, low_cpu_mem_usage=True)\n",
    "\n",
    "tokenizer.src_lang = 'en_XX'\n",
    "tokenizer.tgt_lang = 'fa_IR'\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T10:33:07.857626Z",
     "iopub.status.busy": "2024-10-02T10:33:07.857309Z",
     "iopub.status.idle": "2024-10-02T10:33:33.488380Z",
     "shell.execute_reply": "2024-10-02T10:33:33.487511Z",
     "shell.execute_reply.started": "2024-10-02T10:33:07.857594Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b64f02693cf4e8cb57b8e843d9769f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a816d049cf249789bf9c2fc0079fd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2137 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"category\"])\n",
    "tokenized_valid = validation_dataset.map(preprocess_function, batched=True, remove_columns=[\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T10:33:33.489852Z",
     "iopub.status.busy": "2024-10-02T10:33:33.489557Z",
     "iopub.status.idle": "2024-10-02T10:33:33.496786Z",
     "shell.execute_reply": "2024-10-02T10:33:33.495907Z",
     "shell.execute_reply.started": "2024-10-02T10:33:33.489820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Output: {'input_ids': [250004, 3293, 83, 10, 3034, 149357, 100, 47, 1098, 47691, 5, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Token IDs: [250004, 3293, 83, 10, 3034, 149357, 100, 47, 1098, 47691, 5, 2]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Tokens: ['en_XX', '▁This', '▁is', '▁a', '▁test', '▁sentence', '▁for', '▁to', 'ken', 'ization', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = \"This is a test sentence for tokenization.\"\n",
    "\n",
    "tokenized_output = tokenizer(sample_sentence)\n",
    "\n",
    "print(\"Tokenized Output:\", tokenized_output)\n",
    "print(\"Token IDs:\", tokenized_output['input_ids'])\n",
    "print(\"Attention Mask:\", tokenized_output['attention_mask'])\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_output['input_ids'])\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T10:33:33.498254Z",
     "iopub.status.busy": "2024-10-02T10:33:33.497904Z",
     "iopub.status.idle": "2024-10-02T10:33:33.507920Z",
     "shell.execute_reply": "2024-10-02T10:33:33.507193Z",
     "shell.execute_reply.started": "2024-10-02T10:33:33.498223Z"
    }
   },
   "outputs": [],
   "source": [
    "# Freeze all encoder layers\n",
    "# for param in model.model.encoder.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Unfreeze the last 5 layers of the encoder\n",
    "# for param in model.model.encoder.layers[-5:].parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# Now the model's last 5 encoder layers are trainable, and the rest are frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T10:33:33.509416Z",
     "iopub.status.busy": "2024-10-02T10:33:33.508978Z",
     "iopub.status.idle": "2024-10-02T10:33:33.673101Z",
     "shell.execute_reply": "2024-10-02T10:33:33.672321Z",
     "shell.execute_reply.started": "2024-10-02T10:33:33.509365Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T10:33:33.674606Z",
     "iopub.status.busy": "2024-10-02T10:33:33.674293Z",
     "iopub.status.idle": "2024-10-02T13:01:15.697593Z",
     "shell.execute_reply": "2024-10-02T13:01:15.696574Z",
     "shell.execute_reply.started": "2024-10-02T10:33:33.674573Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/888283462.py:30: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "  0%|          | 0/6250 [00:00<?, ?it/s]/tmp/ipykernel_30/888283462.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "100%|██████████| 6250/6250 [1:13:00<00:00,  1.43it/s]\n",
      "/tmp/ipykernel_30/888283462.py:84: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "100%|██████████| 6250/6250 [1:12:58<00:00,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "writer = SummaryWriter(log_dir='./logs')\n",
    "\n",
    "logging.basicConfig(filename='./logs/training.log',\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s - %(message)s', \n",
    "                    level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_valid.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = torch.utils.data.DataLoader(tokenized_train, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(tokenized_valid, batch_size=batch_size)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "accumulation_steps = 2\n",
    "num_epochs = 2\n",
    "\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "\n",
    "checkpoint_dir = '/kaggle/working/'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    logger.info(f\"Epoch {epoch + 1}/{num_epochs} started\")\n",
    "\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, batch in enumerate(tqdm(train_dataloader), start=1):\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(input_ids=inputs, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss / accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_train_loss += loss.item() * accumulation_steps\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            logger.info(f\"Step {step}: Training loss = {loss.item()}\")\n",
    "            writer.add_scalar('Training Loss', loss.item(), global_step=step + (epoch * len(train_dataloader)))\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    logger.info(f\"Training loss after epoch {epoch + 1}: {avg_train_loss}\")\n",
    "    writer.add_scalar('Average Training Loss', avg_train_loss, global_step=epoch)\n",
    "\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    for batch in valid_dataloader:\n",
    "        with torch.no_grad():\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(input_ids=inputs, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "    avg_eval_loss = total_eval_loss / len(valid_dataloader)\n",
    "    logger.info(f\"Validation loss after epoch {epoch + 1}: {avg_eval_loss}\")\n",
    "    writer.add_scalar('Validation Loss', avg_eval_loss, global_step=epoch)\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': avg_train_loss,\n",
    "        'valid_loss': avg_eval_loss,\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_dir + f'model_epoch_{epoch + 1}.pth')\n",
    "    logger.info(f\"Checkpoint saved for epoch {epoch + 1}\")\n",
    "\n",
    "final_model_path = checkpoint_dir + 'final_model.pth'\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "logger.info(\"Final model saved for inference.\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T13:02:45.270475Z",
     "iopub.status.busy": "2024-10-02T13:02:45.269657Z",
     "iopub.status.idle": "2024-10-02T13:02:57.901543Z",
     "shell.execute_reply": "2024-10-02T13:02:57.900218Z",
     "shell.execute_reply.started": "2024-10-02T13:02:45.270434Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mega.py\n",
      "  Downloading mega.py-1.0.8-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: requests>=0.10 in /opt/conda/lib/python3.10/site-packages (from mega.py) (2.32.3)\n",
      "Requirement already satisfied: pycryptodome<4.0.0,>=3.9.6 in /opt/conda/lib/python3.10/site-packages (from mega.py) (3.20.0)\n",
      "Collecting pathlib==1.0.1 (from mega.py)\n",
      "  Downloading pathlib-1.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tenacity<6.0.0,>=5.1.5 (from mega.py)\n",
      "  Downloading tenacity-5.1.5-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=0.10->mega.py) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=0.10->mega.py) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=0.10->mega.py) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=0.10->mega.py) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from tenacity<6.0.0,>=5.1.5->mega.py) (1.16.0)\n",
      "Downloading mega.py-1.0.8-py2.py3-none-any.whl (19 kB)\n",
      "Downloading pathlib-1.0.1-py3-none-any.whl (14 kB)\n",
      "Downloading tenacity-5.1.5-py2.py3-none-any.whl (34 kB)\n",
      "Installing collected packages: pathlib, tenacity, mega.py\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.3.0\n",
      "    Uninstalling tenacity-8.3.0:\n",
      "      Successfully uninstalled tenacity-8.3.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "plotly 5.22.0 requires tenacity>=6.2.0, but you have tenacity 5.1.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed mega.py-1.0.8 pathlib-1.0.1 tenacity-5.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install mega.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T13:09:06.178104Z",
     "iopub.status.busy": "2024-10-02T13:09:06.177018Z",
     "iopub.status.idle": "2024-10-02T13:09:07.355351Z",
     "shell.execute_reply": "2024-10-02T13:09:07.354511Z",
     "shell.execute_reply.started": "2024-10-02T13:09:06.178037Z"
    }
   },
   "outputs": [],
   "source": [
    "from mega import Mega\n",
    "\n",
    "mega = Mega()\n",
    "\n",
    "email = 'abtin.mansouri2003@gmail.com'\n",
    "password = 'zyszid-4rygro-tImkeq'\n",
    "\n",
    "m = mega.login(email, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-02T14:51:50.982153Z",
     "iopub.status.idle": "2024-10-02T14:51:50.982634Z",
     "shell.execute_reply": "2024-10-02T14:51:50.982405Z",
     "shell.execute_reply.started": "2024-10-02T14:51:50.982379Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = '/kaggle/working/final_model.pth'\n",
    "\n",
    "file = m.upload(file_path)\n",
    "\n",
    "link = m.get_upload_link(file)\n",
    "\n",
    "print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:33:57.120877Z",
     "iopub.status.busy": "2024-10-02T15:33:57.120554Z",
     "iopub.status.idle": "2024-10-02T15:34:11.106294Z",
     "shell.execute_reply": "2024-10-02T15:34:11.105311Z",
     "shell.execute_reply.started": "2024-10-02T15:33:57.120842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  for what feels like an eternity.\n",
      "Translated Text: براي چيزي كه مثل يک ابديت احساس ميشه\n"
     ]
    }
   ],
   "source": [
    "def translate(text, tokenizer, model, source_lang='en_XX', target_lang='fa_IR', device='cpu', max_length=512):\n",
    "    tokenizer.src_lang = source_lang\n",
    "    tokenizer.tgt_lang = target_lang\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=max_length, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=max_length,\n",
    "            num_beams=10,\n",
    "            length_penalty=2.0,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    translation = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "    return translation\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = '/kaggle/working/final_model.pth'\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    tokenizer, model = load_model_and_tokenizer(model_path)\n",
    "    model.to(device)\n",
    "\n",
    "    text = \" for what feels like an eternity.\"\n",
    "    translation = translate(text, tokenizer, model, source_lang='en_XX', target_lang='fa_IR', device=device)\n",
    "    print(f\"Original Text: {text}\")\n",
    "    print(f\"Translated Text: {translation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
